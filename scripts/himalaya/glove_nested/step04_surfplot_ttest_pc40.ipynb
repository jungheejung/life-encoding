{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook identifies significant r values\n",
    "* fisherZ correct\n",
    "* stack across participants and runs\n",
    "* run t-test and identify r values greater than 0\n",
    "* reconvert fisherZ back to correlation coefficients\n",
    "* stack coefficients across hemisphere for multiple comparisons\n",
    "* save results in corresponding hemisphere while preserving medial/non-medial wall vertices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Aug/2024\n",
    "* comb-r2\n",
    "* Need to grab comb-r2 instead of agents-r or bg-r."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import scipy.stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import nibabel as nib\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: write_gifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_gifti_v2(data, output_fn, template_fn):\n",
    "    gii = nib.load(template_fn)\n",
    "    for i in np.arange(gii.numDA):\n",
    "        gii.remove_gifti_data_array(0)\n",
    "    gda = nib.gifti.GiftiDataArray(data)\n",
    "    gii.add_gifti_data_array(gda)\n",
    "    # nib.gifti.write(gii, output_fn)\n",
    "    gii.to_filename(output_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters for Nested \"actions bg moten\" -- isolate \"agents\"\n",
    "TODO: \n",
    "1. load comb-r2 for full model & comb-r2 for glove_nested_actions-bg-moten -> \n",
    "   * uniq = (comb_r2_full - comb_r2_nested) / comb_r2_full\n",
    "2. Do this iteratively for all nested models (total of 3)\n",
    "3. t-tests of uniq against 0. \n",
    "4. brain map: masked unique proportion R2, based on fdr-corrected t values. (Expected to be similar to Figure 1. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma_dir = '/Users/h/suma-fsaverage6'\n",
    "main_dir = '/dartfs/rc/lab/D/DBIC/DBIC/f0042x1/life-encoding'\n",
    "main_dir = '/Volumes/life-encoding'\n",
    "features = 'actionsmoten'\n",
    "\n",
    "ANALYSIS=\"glove_nested_actions-bg-moten\" # agents\n",
    "# \"glove_nested_agents-bg-moten\"\n",
    "# \"glove_nested_actions-agents-moten\"\n",
    "\n",
    "output_dir = os.path.join(main_dir, 'results', 'himalaya', 'glove_nested', 'actions-bg-moten', 'ha_common_pca-40')\n",
    "alignment = 'ha_common'\n",
    "results = ['comb-r2']#,  'moten-r']\n",
    "runs = [1, 2, 3, 4]\n",
    "hemis = ['lh', 'rh']\n",
    "pca_comp = 40\n",
    "fmri_durs = {1: 374, 2: 346, 3: 377, 4: 412}\n",
    "n_samples = 1509\n",
    "n_vertices = 40962\n",
    "n_medial = {'lh': 3486, 'rh': 3491}\n",
    "subjects = ['sub-rid000001', 'sub-rid000005', 'sub-rid000006',\n",
    "            'sub-rid000009', 'sub-rid000012', 'sub-rid000014',\n",
    "            'sub-rid000017', 'sub-rid000019', 'sub-rid000024',\n",
    "            'sub-rid000027', 'sub-rid000031', 'sub-rid000032',\n",
    "            'sub-rid000033', 'sub-rid000034', 'sub-rid000036',\n",
    "            'sub-rid000037', 'sub-rid000038', 'sub-rid000041']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute one sample t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    hemi_t = []\n",
    "    hemi_p = []\n",
    "    hemi_mean = []\n",
    "    print(f\"starting {result} ________________\")\n",
    "    for hemisphere in hemis:\n",
    "        medial_mask = np.load(os.path.join(main_dir, 'data', f'fsaverage6_medial_{hemisphere}.npy'))\n",
    "        assert np.sum(medial_mask) == n_medial[hemisphere]\n",
    "        cortical_vertices = ~medial_mask # boolean (true for non-medials, false for medials)\n",
    "        cortical_coords = np.where(cortical_vertices)[0] # returns indices of non-medials\n",
    "        avg_all = []\n",
    "        # NOTE 1. Fisher z transform per run per participant (np.arctanh)\n",
    "        for test_subject in subjects:\n",
    "            stack_fisherz_run = []\n",
    "            for test_run in runs:           \n",
    "\n",
    "                run_data = np.load(f\"{output_dir}/{result}_pca-{pca_comp}_align-{alignment}_{test_subject}_run-{test_run}_hemi-{hemisphere}.npy\")\n",
    "                fisherz_run = np.arctanh(run_data[0, cortical_vertices])\n",
    "                stack_fisherz_run.append(fisherz_run)\n",
    "        \n",
    "            # NOTE: 2. average (z-transformed) correlations across runs: yields 18 maps (1 per subject)\n",
    "            avg_run = np.mean(np.vstack(stack_fisherz_run), axis = 0)\n",
    "            avg_all.append(avg_run)\n",
    "        fisherz_all = np.vstack(avg_all) # 18 x nonmedial (per hemi)\n",
    "\n",
    "        # NOTE: 3. Scipy ttest_1samp to get t-value and p-value\n",
    "        t, p = scipy.stats.ttest_1samp(fisherz_all, popmean=0,axis=0, alternative='greater')\n",
    "        hemi_t.append(t)\n",
    "        hemi_p.append(p)\n",
    "        hemi_mean.append(np.tanh(np.nanmean(fisherz_all, axis = 0)))\n",
    "\n",
    "    # %% NOTE: 4-1. concatenate (np.hstack) the two hemispheres p-values (and exclude medial wall) prior to computing FDR (load in cortical_vertices.npy)\n",
    "    left_vert = hemi_p[0].shape[0]\n",
    "    t_all = np.hstack(hemi_t)\n",
    "    p_all = np.hstack(hemi_p)\n",
    "\n",
    "    reject, q_all, _, _ = multipletests(p_all, method = 'fdr_bh')\n",
    "    q_both = [q_all[:left_vert], q_all[left_vert:]]\n",
    "    print(\"* completed t-tests for both hemispheres\")\n",
    "    hemilabels = ['lh', 'rh']\n",
    "    Path(join(output_dir, 'stats')).mkdir(parents = True, exist_ok = True)\n",
    "    for h, hemisphere in enumerate(hemis):\n",
    "        stats = np.zeros((n_vertices))\n",
    "        hemi_mean[h][q_both[h] >= .05] = 0\n",
    "        medial_mask = np.load(os.path.join(main_dir, 'data', f'fsaverage6_medial_{hemisphere}.npy'))\n",
    "        assert np.sum(medial_mask) == n_medial[hemisphere]\n",
    "        cortical_vertices = ~medial_mask # boolean (true for non-medials, false for medials)\n",
    "        cortical_coords = np.where(cortical_vertices)[0] # returns indices of non-medials\n",
    "        stats[cortical_coords] = hemi_mean[h]\n",
    "        save_fname = f\"{output_dir}/stats/{result}_pca-{pca_comp}_align-{alignment}_hemi-{hemisphere}_ttest.gii\"\n",
    "        write_gifti_v2(stats.astype(np.float32),\n",
    "        template_fn = os.path.join(suma_dir, f\"{hemilabels[h]}.pial.gii\"), \n",
    "        output_fn = save_fname)\n",
    "    print(\"* saved output (thresholded average r) for both hemispheres\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute paired sample t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "results = ['actions-r', 'bg-r',  'agents-r']\n",
    "pairs = list(combinations(results, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for p1, p2 in pairs:\n",
    "    hemi_t = []\n",
    "    hemi_p = []\n",
    "    hemi_mean = []\n",
    "    print(f\"starting {p1} {p2}________________\")\n",
    "    for hemisphere in hemis:\n",
    "        medial_mask = np.load(os.path.join(main_dir, 'data', f'fsaverage6_medial_{hemisphere}.npy'))\n",
    "        assert np.sum(medial_mask) == n_medial[hemisphere]\n",
    "        cortical_vertices = ~medial_mask # boolean (true for non-medials, false for medials)\n",
    "        cortical_coords = np.where(cortical_vertices)[0] # returns indices of non-medials\n",
    "        avg_all = []\n",
    "        \n",
    "        # NOTE 1. Fisher z transform per run per participant (np.arctanh)\n",
    "        for test_subject in subjects:\n",
    "            stack_fisherz_p1run = []\n",
    "            stack_fisherz_p2run = []\n",
    "            # stack_p1run = []\n",
    "            # stack_p2run = []\n",
    "            for test_run in runs:           \n",
    "                p1_data = np.load(f\"{output_dir}/{p1}_pca-{pca_comp}_align-{alignment}_{test_subject}_run-{test_run}_hemi-{hemisphere}.npy\")\n",
    "                fisherz_p1run = np.arctanh(p1_data[0, cortical_vertices])\n",
    "                stack_fisherz_p1run.append(fisherz_p1run)\n",
    "\n",
    "                p2_data = np.load(f\"{output_dir}/{p2}_pca-{pca_comp}_align-{alignment}_{test_subject}_run-{test_run}_hemi-{hemisphere}.npy\")\n",
    "                fisherz_p2run = np.arctanh(p2_data[0, cortical_vertices])\n",
    "                stack_fisherz_p2run.append(fisherz_p2run)\n",
    "        \n",
    "            # NOTE: 2. average (z-transformed) correlations across runs: yields 18 maps (1 per subject)\n",
    "            # avg_run = np.mean(np.vstack(stack_fisherz_run), axis = 0)\n",
    "            stack_p1run = np.vstack(stack_fisherz_p1run)\n",
    "            stack_p2run = np.vstack(stack_fisherz_p2run)\n",
    "            diff = stack_p1run - stack_p2run\n",
    "            # print(stack_run.shape)\n",
    "            # raise\n",
    "            avg_run = np.mean(diff, axis = 0)\n",
    "            avg_all.append(avg_run)\n",
    "        fisherz_all = np.vstack(avg_all) # 18 x nonmedial (per hemi)\n",
    "\n",
    "        # NOTE: 3. Scipy ttest_1samp to get t-value and p-value\n",
    "        t, p = scipy.stats.ttest_1samp(fisherz_all, popmean=0,axis=0, alternative='two-sided')\n",
    "        hemi_t.append(t)\n",
    "        hemi_p.append(p)\n",
    "        hemi_mean.append(np.tanh(np.nanmean(fisherz_all, axis = 0)))\n",
    "\n",
    "    # %% NOTE: 4-1. concatenate (np.hstack) the two hemispheres p-values (and exclude medial wall) prior to computing FDR (load in cortical_vertices.npy)\n",
    "    left_vert = hemi_p[0].shape[0]\n",
    "    t_all = np.hstack(hemi_t)\n",
    "    p_all = np.hstack(hemi_p)\n",
    "\n",
    "    reject, q_all, _, _ = multipletests(p_all, method = 'fdr_bh')\n",
    "    q_both = [q_all[:left_vert], q_all[left_vert:]]\n",
    "    print(\"* completed t-tests for both hemispheres\")\n",
    "    hemilabels = ['lh', 'rh']\n",
    "    Path(join(output_dir, 'stats')).mkdir(parents = True, exist_ok = True)\n",
    "    for h, hemisphere in enumerate(hemis):\n",
    "        stats = np.zeros((n_vertices))\n",
    "        hemi_mean[h][q_both[h] >= .05] = 0\n",
    "        medial_mask = np.load(os.path.join(main_dir, 'data', f'fsaverage6_medial_{hemisphere}.npy'))\n",
    "        assert np.sum(medial_mask) == n_medial[hemisphere]\n",
    "        cortical_vertices = ~medial_mask # boolean (true for non-medials, false for medials)\n",
    "        cortical_coords = np.where(cortical_vertices)[0] # returns indices of non-medials\n",
    "        stats[cortical_coords] = hemi_mean[h]\n",
    "        save_fname = f\"{output_dir}/stats/{p1[:-2]}-{p2[:-2]}_pca-{pca_comp}_align-{alignment}_hemi-{hemisphere}_ttest.gii\"\n",
    "        write_gifti_v2(stats.astype(np.float32),\n",
    "        template_fn = os.path.join(suma_dir, f\"{hemilabels[h]}.pial.gii\"), \n",
    "        output_fn = save_fname)\n",
    "    print(\"* saved output (thresholded average r) for both hemispheres\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot results in fslr\n",
    "\n",
    "plot t-test results in surfplots, Freesurfer surface plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromaps.datasets import fetch_fsaverage, fetch_fslr\n",
    "import numpy as np\n",
    "from surfplot import Plot\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from os.path import join\n",
    "import nibabel as nib\n",
    "surfaces = fetch_fsaverage()\n",
    "lh, rh = surfaces['inflated']\n",
    "# main_dir = '/dartfs/rc/lab/D/DBIC/DBIC/f0042x1/life-encoding\n",
    "main_dir = '/Volumes/life-encoding'\n",
    "# output_dir = os.path.join(main_dir, 'results', 'himalaya', f\"single-{features}\", 'ha_common_pca-40')\n",
    "gii_dir = join(output_dir, 'stats')\n",
    "print(gii_dir)\n",
    "alignment = 'ha_common'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = 'bg-r'\n",
    "# # bgnib = nib.load(join(gii_dir,f'{key}_align-{alignment}_hemi-lh_ttest.gii'))\n",
    "# bgLfname = f\"{gii_dir}/{key}_pca-40_align-{alignment}_hemi-lh_ttest.gii\"\n",
    "# bgRfname = f\"{gii_dir}/{key}_pca-40_align-{alignment}_hemi-rh_ttest.gii\"\n",
    "# bgL = nib.load(f\"{gii_dir}/{key}_pca-40_align-{alignment}_hemi-lh_ttest.gii\")\n",
    "# bgR = nib.load(f\"{gii_dir}/{key}_pca-40_align-{alignment}_hemi-rh_ttest.gii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsaverage_to_fslr_and_plot(Lfname, Rfname, key, cmap='inferno', max=.20):\n",
    "    from neuromaps.datasets import fetch_fslr\n",
    "    from neuromaps.transforms import fsaverage_to_fslr\n",
    "    giiL = nib.load(Lfname)\n",
    "    giiR = nib.load(Rfname)\n",
    "    L_fslr = fsaverage_to_fslr(giiL, target_density='32k', hemi='L', method='linear')\n",
    "    R_fslr = fsaverage_to_fslr(giiR, target_density='32k', hemi='R', method='linear')\n",
    "    \n",
    "    surfaces_fslr = fetch_fslr()\n",
    "    lh_fslr, rh_fslr = surfaces_fslr['inflated']\n",
    "\n",
    "    color_range = (0,max)\n",
    "    p = Plot(surf_lh=lh_fslr, \n",
    "             surf_rh=rh_fslr, \n",
    "             size=(1000, 200), \n",
    "             zoom=1.2, layout='row', \n",
    "             views=['lateral', 'medial', 'ventral', 'posterior'], \n",
    "             mirror_views=True, brightness=.7)\n",
    "    p.add_layer({'left': L_fslr[0], \n",
    "                'right': R_fslr[0]}, \n",
    "                cmap=cmap, cbar=True,\n",
    "                color_range=color_range,\n",
    "                cbar_label=key\n",
    "                ) # YlOrRd_r\n",
    "    cbar_kws = dict(outer_labels_only=True, pad=.02, n_ticks=2, decimals=3)\n",
    "    fig = p.build(cbar_kws=cbar_kws)\n",
    "    # fig.show()\n",
    "    return(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gii_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment = 'ha_common'\n",
    "# ['bg', 'agents', 'actions', 'moten']\n",
    "for feature in ['bg', 'agents', 'actions', 'moten']:\n",
    "    bgLfname = f\"{gii_dir}/{feature}-r_pca-40_align-{alignment}_hemi-lh_ttest.gii\"\n",
    "    bgRfname = f\"{gii_dir}/{feature}-r_pca-40_align-{alignment}_hemi-rh_ttest.gii\"\n",
    "    fsaverage_to_fslr_and_plot(bgLfname, bgRfname, feature, cmap='inferno', max=.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count significant voxels\n",
    "How many voxels are significant per stat map (FDR-corrected)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment = 'ha_common'\n",
    "# ['bg', 'agents', 'actions', 'moten']\n",
    "for feature in ['bg', 'agents', 'actions', 'moten']:\n",
    "# feature = 'bg'\n",
    "    Lfname = f\"{gii_dir}/{feature}-r_pca-40_align-{alignment}_hemi-lh_ttest.gii\"\n",
    "    Rfname = f\"{gii_dir}/{feature}-r_pca-40_align-{alignment}_hemi-rh_ttest.gii\"\n",
    "    # fsaverage_to_fslr_and_plot(bgLfname, bgRfname, feature, cmap='inferno', max=.2)\n",
    "\n",
    "    giiL = nib.load(Lfname)\n",
    "    giiR = nib.load(Rfname)\n",
    "\n",
    "    sig_voxel = np.sum(giiL.agg_data() > 0) + np.sum(giiR.agg_data() > 0) \n",
    "    print(f\"{feature}: {sig_voxel}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster thresholded\n",
    "\n",
    "We threshold the maps using work bench. The output is a (semi) boolean mask, thereby, we apply the mask back onto the t-test maps. \n",
    "Steps include\n",
    "* Workbench cluster threshold\n",
    "* multiply workbench mask with t-test maps\n",
    "* plot thresholded maps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workbench cluster threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "# parameteres\n",
    "hemis = ['lh', 'rh']\n",
    "features = ['bg', 'agents', 'actions', 'moten']\n",
    "clustersize = 100\n",
    "hemi_dict = {'lh': 'L', 'rh': 'R'}\n",
    "fsaverage_dir = '/Users/h/neuromaps-data/atlases/fsaverage'\n",
    "data_dir = '/Volumes/life-encoding/results/himalaya/glove/ha_common_pca-40/stats'\n",
    "\n",
    "# thresholding via workbench\n",
    "for hemi in hemis:\n",
    "    for feature in features:\n",
    "        fsaverage_template = f\"{fsaverage_dir}/tpl-fsaverage_den-41k_hemi-{hemi_dict[hemi]}_inflated.surf.gii\"\n",
    "        himalaya = f\"{data_dir}/{feature}-r_pca-40_align-ha_common_hemi-{hemi}_ttest.gii\"\n",
    "        outputfname = f\"{data_dir}/{feature}-r_pca-40_align-ha_common_hemi-{hemi}_ttest_cluster-{clustersize}.gii\"\n",
    "        return_code = call(f\"wb_command -metric-find-clusters {fsaverage_template} {himalaya} 0 {clustersize} {outputfname}\", shell=True)\n",
    "print(f\"Return: {return_code}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check cluster maps\n",
    "The images will look like masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['bg', 'agents', 'actions', 'moten']:\n",
    "    clusterthresL = f\"{data_dir}/{feature}-r_pca-40_align-ha_common_hemi-lh_ttest_cluster-{clustersize}.gii\"\n",
    "    clusterthresR = f\"{data_dir}/{feature}-r_pca-40_align-ha_common_hemi-rh_ttest_cluster-{clustersize}.gii\"\n",
    "    fsaverage_to_fslr_and_plot(clusterthresL, clusterthresR, f'{feature}-r', cmap = 'Reds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiply workbench cluster threshold maps to t-test maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustersize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_gifti_v2(data, output_fn, template_fn):\n",
    "    gii = nib.load(template_fn)\n",
    "    for i in np.arange(gii.numDA):\n",
    "        gii.remove_gifti_data_array(0)\n",
    "    gda = nib.gifti.GiftiDataArray(data)\n",
    "    gii.add_gifti_data_array(gda)\n",
    "    # nib.gifti.write(gii, output_fn)\n",
    "    gii.to_filename(output_fn)\n",
    "\n",
    "hemis = ['lh', 'rh']\n",
    "hemi_dict = {'lh': 'L', 'rh': 'R'}\n",
    "features = ['bg', 'agents', 'actions', 'moten']\n",
    "# clustersize = 50\n",
    "\n",
    "fsaverage_dir = '/Users/h/neuromaps-data/atlases/fsaverage'\n",
    "data_dir = '/Volumes/life-encoding/results/himalaya/glove/ha_common_pca-40/stats'\n",
    "\n",
    "# thresholding via workbench\n",
    "for hemi in hemis:\n",
    "    for feature in features:\n",
    "        clustermask_fname = f\"{data_dir}/{feature}-r_pca-40_align-ha_common_hemi-{hemi}_ttest_cluster-{clustersize}.gii\"\n",
    "        clustL = nib.load(clustermask_fname).agg_data() > 0\n",
    "        himalaya_fname = f\"{gii_dir}/{feature}-r_pca-40_align-ha_common_hemi-{hemi}_ttest.gii\"\n",
    "        himalaya = nib.load(himalaya_fname).agg_data()\n",
    "        maskeddata = clustL * himalaya\n",
    "        write_gifti_v2(maskeddata.astype(np.float32), \n",
    "            output_fn=f\"{data_dir}/{feature}-r_pca-40_align-{alignment}_hemi-{hemi}_ttest_thres-cluster{clustersize}.gii\",\n",
    "            template_fn=f\"{fsaverage_dir}/tpl-fsaverage_den-41k_hemi-{hemi_dict[hemi]}_pial.surf.gii\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot thresholded maps in fslr [manuscript]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/h/Documents/projects_local/life-encoding/figure'\n",
    "for feature in ['bg', 'agents', 'actions', 'moten']:\n",
    "    thresL = f\"{data_dir}/{feature}-r_pca-40_align-{alignment}_hemi-lh_ttest_thres-cluster{clustersize}.gii\"\n",
    "    thresR = f\"{data_dir}/{feature}-r_pca-40_align-{alignment}_hemi-rh_ttest_thres-cluster{clustersize}.gii\"\n",
    "    fig = fsaverage_to_fslr_and_plot(thresL, thresR, f'{feature}-r', cmap = 'inferno', max=.2)\n",
    "    fig.savefig(join(save_dir, f\"{feature}-r_pca-40_align-{alignment}_hemi-lh_ttest_thres-cluster{clustersize}.png\"), dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# contrast maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "results = ['actions-r', 'bg-r',  'agents-r']\n",
    "pairs = list(combinations(results, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_num = 40\n",
    "for p1, p2 in pairs:\n",
    "    hemi_t = []\n",
    "    hemi_p = []\n",
    "    hemi_mean = []\n",
    "    print(f\"starting {p1} {p2}________________\")\n",
    "    for hemisphere in hemis:\n",
    "        medial_mask = np.load(os.path.join(main_dir, 'data', f'fsaverage6_medial_{hemisphere}.npy'))\n",
    "        assert np.sum(medial_mask) == n_medial[hemisphere]\n",
    "        cortical_vertices = ~medial_mask # boolean (true for non-medials, false for medials)\n",
    "        cortical_coords = np.where(cortical_vertices)[0] # returns indices of non-medials\n",
    "        avg_all = []\n",
    "        \n",
    "        # NOTE 1. Fisher z transform per run per participant (np.arctanh)\n",
    "        for test_subject in subjects:\n",
    "            stack_fisherz_p1run = []\n",
    "            stack_fisherz_p2run = []\n",
    "            # stack_p1run = []\n",
    "            # stack_p2run = []\n",
    "            for test_run in runs:           \n",
    "                p1_data = np.load(f\"{output_dir}/{p1}_pca-{pca_num}_align-{alignment}_{test_subject}_run-{test_run}_hemi-{hemisphere}.npy\")\n",
    "                fisherz_p1run = np.arctanh(p1_data[0, cortical_vertices])\n",
    "                stack_fisherz_p1run.append(fisherz_p1run)\n",
    "\n",
    "                p2_data = np.load(f\"{output_dir}/{p2}_pca-{pca_num}_align-{alignment}_{test_subject}_run-{test_run}_hemi-{hemisphere}.npy\")\n",
    "                fisherz_p2run = np.arctanh(p2_data[0, cortical_vertices])\n",
    "                stack_fisherz_p2run.append(fisherz_p2run)\n",
    "        \n",
    "            # NOTE: 2. average (z-transformed) correlations across runs: yields 18 maps (1 per subject)\n",
    "            # avg_run = np.mean(np.vstack(stack_fisherz_run), axis = 0)\n",
    "            stack_p1run = np.vstack(stack_fisherz_p1run)\n",
    "            stack_p2run = np.vstack(stack_fisherz_p2run)\n",
    "            diff = stack_p1run - stack_p2run\n",
    "            # print(stack_run.shape)\n",
    "            # raise\n",
    "            avg_run = np.mean(diff, axis = 0)\n",
    "            avg_all.append(avg_run)\n",
    "        fisherz_all = np.vstack(avg_all) # 18 x nonmedial (per hemi)\n",
    "\n",
    "        # NOTE: 3. Scipy ttest_1samp to get t-value and p-value\n",
    "        t, p = scipy.stats.ttest_1samp(fisherz_all, popmean=0,axis=0, alternative='two-sided')\n",
    "        hemi_t.append(t)\n",
    "        hemi_p.append(p)\n",
    "        hemi_mean.append(np.tanh(np.nanmean(fisherz_all, axis = 0)))\n",
    "\n",
    "    # %% NOTE: 4-1. concatenate (np.hstack) the two hemispheres p-values (and exclude medial wall) prior to computing FDR (load in cortical_vertices.npy)\n",
    "    left_vert = hemi_p[0].shape[0]\n",
    "    t_all = np.hstack(hemi_t)\n",
    "    p_all = np.hstack(hemi_p)\n",
    "\n",
    "    reject, q_all, _, _ = multipletests(p_all, method = 'fdr_bh')\n",
    "    q_both = [q_all[:left_vert], q_all[left_vert:]]\n",
    "    print(\"* completed t-tests for both hemispheres\")\n",
    "    hemilabels = ['lh', 'rh']\n",
    "    Path(join(output_dir, 'stats')).mkdir(parents = True, exist_ok = True)\n",
    "    for h, hemisphere in enumerate(hemis):\n",
    "        stats = np.zeros((n_vertices))\n",
    "        hemi_mean[h][q_both[h] >= .05] = 0\n",
    "        medial_mask = np.load(os.path.join(main_dir, 'data', f'fsaverage6_medial_{hemisphere}.npy'))\n",
    "        assert np.sum(medial_mask) == n_medial[hemisphere]\n",
    "        cortical_vertices = ~medial_mask # boolean (true for non-medials, false for medials)\n",
    "        cortical_coords = np.where(cortical_vertices)[0] # returns indices of non-medials\n",
    "        stats[cortical_coords] = hemi_mean[h]\n",
    "        save_fname = f\"{output_dir}/stats/{p1[:-2]}-{p2[:-2]}_pca-{pca_num}_align-{alignment}_hemi-{hemisphere}_ttest.gii\"\n",
    "        write_gifti_v2(stats.astype(np.float32),\n",
    "        template_fn = os.path.join(suma_dir, f\"{hemilabels[h]}.pial.gii\"), \n",
    "        output_fn = save_fname)\n",
    "    print(\"* saved output (thresholded average r) for both hemispheres\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster correct contrast maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsaverage_to_fslr_contrast(Lfname, Rfname, key, cmap='inferno', max=.20):\n",
    "    from neuromaps.datasets import fetch_fslr\n",
    "    from neuromaps.transforms import fsaverage_to_fslr\n",
    "    giiL = nib.load(Lfname)\n",
    "    giiR = nib.load(Rfname)\n",
    "    L_fslr = fsaverage_to_fslr(giiL, target_density='32k', hemi='L', method='linear')\n",
    "    R_fslr = fsaverage_to_fslr(giiR, target_density='32k', hemi='R', method='linear')\n",
    "    \n",
    "    surfaces_fslr = fetch_fslr()\n",
    "    lh_fslr, rh_fslr = surfaces_fslr['inflated']\n",
    "\n",
    "    color_range = (-1*max,max)\n",
    "    p = Plot(surf_lh=lh_fslr, \n",
    "             surf_rh=rh_fslr, \n",
    "             size=(1000, 200), \n",
    "             zoom=1.2, layout='row', \n",
    "             views=['lateral', 'medial', 'ventral', 'posterior'], \n",
    "             mirror_views=True, brightness=.7)\n",
    "    p.add_layer({'left': L_fslr[0], \n",
    "                'right': R_fslr[0]}, \n",
    "                cmap=cmap, cbar=True,\n",
    "                color_range=color_range,\n",
    "                cbar_label=key\n",
    "                ) # YlOrRd_r\n",
    "    cbar_kws = dict(outer_labels_only=True, pad=.02, n_ticks=2, decimals=3)\n",
    "    fig = p.build(cbar_kws=cbar_kws)\n",
    "    # fig.show()\n",
    "    return(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gii_dir = join(output_dir, 'stats')\n",
    "gii_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "# parameteres\n",
    "hemis = ['lh', 'rh']\n",
    "clustersize = 100\n",
    "hemi_dict = {'lh': 'L', 'rh': 'R'}\n",
    "fsaverage_dir = '/Users/h/neuromaps-data/atlases/fsaverage'\n",
    "data_dir = '/Volumes/life-encoding/results/himalaya/glove/ha_common_pca-40/stats'\n",
    "\n",
    "# thresholding via workbench\n",
    "for hemi in hemis:\n",
    "    # for feature in features:\n",
    "    for key1, key2 in pairs:\n",
    "        fsaverage_template = f\"{fsaverage_dir}/tpl-fsaverage_den-41k_hemi-{hemi_dict[hemi]}_inflated.surf.gii\"\n",
    "        himalaya = f\"{data_dir}/{key1[:-2]}-{key2[:-2]}_pca-40_align-ha_common_hemi-{hemi}_ttest.gii\"\n",
    "        outputfname = f\"{data_dir}/{key1[:-2]}-{key2[:-2]}_pca-40_align-ha_common_hemi-{hemi}_ttest_cluster-{clustersize}.gii\"\n",
    "        return_code = call(f\"wb_command -metric-find-clusters {fsaverage_template} {himalaya} 0 {clustersize} {outputfname}\", shell=True)\n",
    "print(f\"Return: {return_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_surf\n",
    "from nilearn.plotting import plot_surf_stat_map\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import datasets\n",
    "from matplotlib import colors\n",
    "fsaverage = datasets.fetch_surf_fsaverage(mesh = 'fsaverage6')\n",
    "gii_dir = join(output_dir, 'stats')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiply cluster-thresholded boolean mask and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for hemi in hemis:\n",
    "    for key1, key2 in pairs:\n",
    "        clustermask_fname = f\"{data_dir}/{key1[:-2]}-{key2[:-2]}_pca-40_align-ha_common_hemi-{hemi}_ttest_cluster-{clustersize}.gii\"\n",
    "        clustL = nib.load(clustermask_fname).agg_data() > 0\n",
    "        himalaya_fname = f\"{gii_dir}/{key1[:-2]}-{key2[:-2]}_pca-40_align-ha_common_hemi-{hemi}_ttest.gii\"\n",
    "        himalaya = nib.load(himalaya_fname).agg_data()\n",
    "        maskeddata = clustL * himalaya\n",
    "        write_gifti_v2(maskeddata.astype(np.float32), \n",
    "            output_fn=f\"{data_dir}/{key1[:-2]}-{key2[:-2]}_pca-40_align-{alignment}_hemi-{hemi}_ttest_thres-cluster{clustersize}.gii\",\n",
    "            template_fn=f\"{fsaverage_dir}/tpl-fsaverage_den-41k_hemi-{hemi_dict[hemi]}_pial.surf.gii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = 'actions'; key2 =  'bg'\n",
    "contrastL = join(data_dir, f'{key1}-{key2}_pca-40_align-{alignment}_hemi-lh_ttest_thres-cluster{clustersize}.gii')\n",
    "contrastR = join(data_dir, f'{key1}-{key2}_pca-40_align-{alignment}_hemi-rh_ttest_thres-cluster{clustersize}.gii')\n",
    "fig = fsaverage_to_fslr_contrast(contrastL, contrastR, f\"{key1}_{key2}\", cmap = 'coolwarm', max=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = fsaverage_to_fslr_contrast(contrastL, contrastR, f\"{key1}_{key2}\", cmap = 'coolwarm', max=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = 'actions'; key2 =  'agents'\n",
    "contrastL = join(data_dir, f'{key1}-{key2}_pca-40_align-{alignment}_hemi-lh_ttest_thres-cluster{clustersize}.gii')\n",
    "contrastR = join(data_dir, f'{key1}-{key2}_pca-40_align-{alignment}_hemi-rh_ttest_thres-cluster{clustersize}.gii')\n",
    "fig = fsaverage_to_fslr_contrast(contrastL, contrastR, f\"{key1}_{key2}\", cmap = 'inferno', max=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions-agents\n",
    "# actions-bg_\n",
    "# bg-agents\n",
    "\n",
    "key1 = 'bg'; key2 =  'agents'\n",
    "contrastL = join(data_dir, f'{key1}-{key2}_pca-40_align-{alignment}_hemi-lh_ttest_thres-cluster{clustersize}.gii')\n",
    "contrastR = join(data_dir, f'{key1}-{key2}_pca-40_align-{alignment}_hemi-rh_ttest_thres-cluster{clustersize}.gii')\n",
    "fig = fsaverage_to_fslr_contrast(contrastL, contrastR, f\"{key1}_{key2}\", cmap = 'coolwarm', max=.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spacetop_datalad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84c6315c5c633ad37de61bf1803ac3fc9b8c165790b101d56c165935d734a051"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
